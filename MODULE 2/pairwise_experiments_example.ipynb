{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-04T18:11:54.444279Z",
     "start_time": "2025-10-04T18:11:54.433868Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "SET UP\n",
   "id": "fc93d1cd816b9a25"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T18:11:56.596748Z",
     "start_time": "2025-10-04T18:11:55.146146Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "from langsmith import Client\n",
    "client = Client()\n",
    "\n",
    "dataset = client.clone_public_dataset(\"https://smith.langchain.com/public/58c3d6e2-0e5c-425b-bcc2-48b58dee95ce/d\")\n",
    "\n",
    "\n",
    "SUMMARIZATION_SYSTEM_PROMPT =\"\"\"You are a judge tasked with comparing two summaries of a transcript.\n",
    "Decide which summary better captures the key ideas, structure, and tone of the transcript.\n",
    "Focus on accuracy, conciseness, and coverage.\n",
    "Return which one is better: 'A' or 'B'.\"\"\"\n",
    "\n",
    "SUMMARIZATION_HUMAN_PROMPT = \"\"\"\n",
    "[The Meeting Transcript]\n",
    "{transcript}\n",
    "\n",
    "[Summary A]\n",
    "{good_summary}\n",
    "\n",
    "[Summary B]\n",
    "{bad_summary}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SummarizationScore(BaseModel):\n",
    "    score: int = Field(description=\"Either 'A' or 'B' depending on which summary better represents the transcript.\")\n",
    "\n",
    "def summary_score_evaluator(inputs: dict, outputs: dict) -> list:\n",
    "    completion = openai_client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": SUMMARIZATION_SYSTEM_PROMPT,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": SUMMARIZATION_HUMAN_PROMPT.format(\n",
    "                    transcript=inputs[\"question\"],\n",
    "                    summary=outputs.get(\"output\", \"N/A\"),\n",
    "                )}\n",
    "        ],\n",
    "        response_format=SummarizationScore,\n",
    "    )\n",
    "\n",
    "    summary_score = completion.choices[0].message.parsed.score\n",
    "    return {\"key\": \"summary_score\", \"score\": summary_score}"
   ],
   "id": "528e82ef1b0899e2",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T18:19:44.397821Z",
     "start_time": "2025-10-04T18:19:32.411313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prompt One: Good Prompt!\n",
    "def good_summarizer(inputs: dict):\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Summarize the customer's complaint in 2–3 sentences, focusing on the main issue, urgency, and requested resolution. Complaint: {inputs['question']}\"\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "client.evaluate(\n",
    "    good_summarizer,\n",
    "    data=dataset,\n",
    "    evaluators=[summary_score_evaluator],\n",
    "    experiment_prefix=\"Good Summarizer\"\n",
    ")"
   ],
   "id": "2b33c68131fd3425",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Good Summarizer-9fbe0b7c' at:\n",
      "https://smith.langchain.com/o/ca700a49-195a-43a4-baa7-fc421725599d/datasets/60e2c84e-8566-44ef-9612-3a494f3e8842/compare?selectedSessions=d60bafb6-4ac2-4d8d-9e00-db98c7d83713\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Error running evaluator <DynamicRunEvaluator summary_score_evaluator> on run 07bb83e6-51bc-4189-896f-314d8e297210: KeyError('good_summary')\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/var/folders/6d/nrcfk1zj04jb6zj0_p75ydvw0000gn/T/ipykernel_4076/4094284967.py\", line 43, in summary_score_evaluator\n",
      "    \"content\": SUMMARIZATION_HUMAN_PROMPT.format(\n",
      "               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        transcript=inputs[\"question\"],\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        summary=outputs.get(\"output\", \"N/A\"),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )}\n",
      "    ^\n",
      "KeyError: 'good_summary'\n",
      "1it [00:01,  1.44s/it]Error running evaluator <DynamicRunEvaluator summary_score_evaluator> on run 4cfdb55e-4700-4cce-b13a-a43d60b0b3e4: KeyError('good_summary')\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/var/folders/6d/nrcfk1zj04jb6zj0_p75ydvw0000gn/T/ipykernel_4076/4094284967.py\", line 43, in summary_score_evaluator\n",
      "    \"content\": SUMMARIZATION_HUMAN_PROMPT.format(\n",
      "               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        transcript=inputs[\"question\"],\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        summary=outputs.get(\"output\", \"N/A\"),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )}\n",
      "    ^\n",
      "KeyError: 'good_summary'\n",
      "2it [00:03,  1.77s/it]Error running evaluator <DynamicRunEvaluator summary_score_evaluator> on run 97e7607b-90aa-4a88-802c-b9ac93d52d13: KeyError('good_summary')\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/var/folders/6d/nrcfk1zj04jb6zj0_p75ydvw0000gn/T/ipykernel_4076/4094284967.py\", line 43, in summary_score_evaluator\n",
      "    \"content\": SUMMARIZATION_HUMAN_PROMPT.format(\n",
      "               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        transcript=inputs[\"question\"],\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        summary=outputs.get(\"output\", \"N/A\"),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )}\n",
      "    ^\n",
      "KeyError: 'good_summary'\n",
      "3it [00:05,  1.84s/it]Error running evaluator <DynamicRunEvaluator summary_score_evaluator> on run 07c6a78a-b2a0-4008-968f-7e4cffedbfa1: KeyError('good_summary')\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/var/folders/6d/nrcfk1zj04jb6zj0_p75ydvw0000gn/T/ipykernel_4076/4094284967.py\", line 43, in summary_score_evaluator\n",
      "    \"content\": SUMMARIZATION_HUMAN_PROMPT.format(\n",
      "               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        transcript=inputs[\"question\"],\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        summary=outputs.get(\"output\", \"N/A\"),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )}\n",
      "    ^\n",
      "KeyError: 'good_summary'\n",
      "4it [00:07,  1.82s/it]Error running evaluator <DynamicRunEvaluator summary_score_evaluator> on run 4288d600-3942-4d2d-9d1e-3c13e30afb43: KeyError('good_summary')\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/var/folders/6d/nrcfk1zj04jb6zj0_p75ydvw0000gn/T/ipykernel_4076/4094284967.py\", line 43, in summary_score_evaluator\n",
      "    \"content\": SUMMARIZATION_HUMAN_PROMPT.format(\n",
      "               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        transcript=inputs[\"question\"],\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        summary=outputs.get(\"output\", \"N/A\"),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )}\n",
      "    ^\n",
      "KeyError: 'good_summary'\n",
      "5it [00:09,  1.85s/it]Error running evaluator <DynamicRunEvaluator summary_score_evaluator> on run 25695c95-71c1-4219-b6d5-3d878f3bb849: KeyError('good_summary')\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/var/folders/6d/nrcfk1zj04jb6zj0_p75ydvw0000gn/T/ipykernel_4076/4094284967.py\", line 43, in summary_score_evaluator\n",
      "    \"content\": SUMMARIZATION_HUMAN_PROMPT.format(\n",
      "               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        transcript=inputs[\"question\"],\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        summary=outputs.get(\"output\", \"N/A\"),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )}\n",
      "    ^\n",
      "KeyError: 'good_summary'\n",
      "6it [00:10,  1.80s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExperimentResults Good Summarizer-9fbe0b7c>"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>feedback.wrapper</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "      <th>reference.output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm having trouble logging into my account. I'...</td>\n",
       "      <td>The customer is experiencing difficulty loggin...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.433336</td>\n",
       "      <td>02a75041-ee65-4f45-809a-e6895777e2e8</td>\n",
       "      <td>07bb83e6-51bc-4189-896f-314d8e297210</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I ordered a wireless keyboard and mouse combo ...</td>\n",
       "      <td>The customer is concerned about the delayed de...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.988072</td>\n",
       "      <td>275fb6b9-0214-4b25-a0de-ce82516f23f9</td>\n",
       "      <td>4cfdb55e-4700-4cce-b13a-a43d60b0b3e4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I just checked my credit card statement and I'...</td>\n",
       "      <td>The customer is upset about being charged twic...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.925274</td>\n",
       "      <td>381d8ed8-be03-4936-af5d-63f2014f77f2</td>\n",
       "      <td>97e7607b-90aa-4a88-802c-b9ac93d52d13</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I purchased your Premium X500 laptop just 3 we...</td>\n",
       "      <td>The customer is experiencing a screen flickeri...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.780498</td>\n",
       "      <td>942956f9-1fc2-4673-a1c7-d64e3c69389b</td>\n",
       "      <td>07c6a78a-b2a0-4008-968f-7e4cffedbfa1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I received my order yesterday but unfortunatel...</td>\n",
       "      <td>The customer is urgently requesting assistance...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.889281</td>\n",
       "      <td>94a952b8-dbc7-49cc-843c-d329de765087</td>\n",
       "      <td>4288d600-3942-4d2d-9d1e-3c13e30afb43</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How many students qualified for JEE Advanced i...</td>\n",
       "      <td>The customer's inquiry seems to be incorrectly...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.217877</td>\n",
       "      <td>d71164f3-7473-42c9-8bf3-e30413cbd6fc</td>\n",
       "      <td>25695c95-71c1-4219-b6d5-3d878f3bb849</td>\n",
       "      <td>Around 2.2 lakh students qualified for JEE Adv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T18:20:09.756229Z",
     "start_time": "2025-10-04T18:20:00.263436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prompt Two: Worse Prompt!\n",
    "def bad_summarizer(inputs: dict):\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Summarize this in one sentence. {inputs['question']}\"\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "client.evaluate(\n",
    "    bad_summarizer,\n",
    "    data=dataset,\n",
    "    evaluators=[summary_score_evaluator],\n",
    "    experiment_prefix=\"Bad Summarizer\"\n",
    ")"
   ],
   "id": "ec3f8203346e8c6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Bad Summarizer-269f3794' at:\n",
      "https://smith.langchain.com/o/ca700a49-195a-43a4-baa7-fc421725599d/datasets/60e2c84e-8566-44ef-9612-3a494f3e8842/compare?selectedSessions=3598836d-34ec-436b-b8e7-41ce6074f5dc\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Error running evaluator <DynamicRunEvaluator summary_score_evaluator> on run 4e4d429b-a2f1-4f30-b9e8-04e02e0f6332: KeyError('good_summary')\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/var/folders/6d/nrcfk1zj04jb6zj0_p75ydvw0000gn/T/ipykernel_4076/4094284967.py\", line 43, in summary_score_evaluator\n",
      "    \"content\": SUMMARIZATION_HUMAN_PROMPT.format(\n",
      "               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        transcript=inputs[\"question\"],\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        summary=outputs.get(\"output\", \"N/A\"),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )}\n",
      "    ^\n",
      "KeyError: 'good_summary'\n",
      "1it [00:01,  1.03s/it]Error running evaluator <DynamicRunEvaluator summary_score_evaluator> on run 5847e5cc-d51d-4385-9924-63b3a269d9fc: KeyError('good_summary')\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/var/folders/6d/nrcfk1zj04jb6zj0_p75ydvw0000gn/T/ipykernel_4076/4094284967.py\", line 43, in summary_score_evaluator\n",
      "    \"content\": SUMMARIZATION_HUMAN_PROMPT.format(\n",
      "               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        transcript=inputs[\"question\"],\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        summary=outputs.get(\"output\", \"N/A\"),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )}\n",
      "    ^\n",
      "KeyError: 'good_summary'\n",
      "2it [00:02,  1.28s/it]Error running evaluator <DynamicRunEvaluator summary_score_evaluator> on run c5cc41d9-f8ff-4239-b3f1-085d09f72bab: KeyError('good_summary')\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/var/folders/6d/nrcfk1zj04jb6zj0_p75ydvw0000gn/T/ipykernel_4076/4094284967.py\", line 43, in summary_score_evaluator\n",
      "    \"content\": SUMMARIZATION_HUMAN_PROMPT.format(\n",
      "               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        transcript=inputs[\"question\"],\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        summary=outputs.get(\"output\", \"N/A\"),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )}\n",
      "    ^\n",
      "KeyError: 'good_summary'\n",
      "3it [00:04,  1.51s/it]Error running evaluator <DynamicRunEvaluator summary_score_evaluator> on run 4670c6f9-e3cc-436e-be8c-95cf06b52540: KeyError('good_summary')\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/var/folders/6d/nrcfk1zj04jb6zj0_p75ydvw0000gn/T/ipykernel_4076/4094284967.py\", line 43, in summary_score_evaluator\n",
      "    \"content\": SUMMARIZATION_HUMAN_PROMPT.format(\n",
      "               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        transcript=inputs[\"question\"],\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        summary=outputs.get(\"output\", \"N/A\"),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )}\n",
      "    ^\n",
      "KeyError: 'good_summary'\n",
      "4it [00:05,  1.40s/it]Error running evaluator <DynamicRunEvaluator summary_score_evaluator> on run 15cbd39b-9b19-4ec8-a13c-5bce62477d97: KeyError('good_summary')\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/var/folders/6d/nrcfk1zj04jb6zj0_p75ydvw0000gn/T/ipykernel_4076/4094284967.py\", line 43, in summary_score_evaluator\n",
      "    \"content\": SUMMARIZATION_HUMAN_PROMPT.format(\n",
      "               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        transcript=inputs[\"question\"],\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        summary=outputs.get(\"output\", \"N/A\"),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )}\n",
      "    ^\n",
      "KeyError: 'good_summary'\n",
      "5it [00:06,  1.36s/it]Error running evaluator <DynamicRunEvaluator summary_score_evaluator> on run 9b360431-0c9b-48b4-a5df-e2140b163985: KeyError('good_summary')\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/_runner.py\", line 1619, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "        run=run,\n",
      "        example=example,\n",
      "        evaluator_run_id=evaluator_run_id,\n",
      "    )\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "        run,\n",
      "        example,\n",
      "        langsmith_extra={\"run_id\": evaluator_run_id, \"metadata\": metadata},\n",
      "    )\n",
      "  File \"/Users/sarth/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/var/folders/6d/nrcfk1zj04jb6zj0_p75ydvw0000gn/T/ipykernel_4076/4094284967.py\", line 43, in summary_score_evaluator\n",
      "    \"content\": SUMMARIZATION_HUMAN_PROMPT.format(\n",
      "               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        transcript=inputs[\"question\"],\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        summary=outputs.get(\"output\", \"N/A\"),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )}\n",
      "    ^\n",
      "KeyError: 'good_summary'\n",
      "6it [00:08,  1.41s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExperimentResults Bad Summarizer-269f3794>"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>feedback.wrapper</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "      <th>reference.output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm having trouble logging into my account. I'...</td>\n",
       "      <td>I'm unable to log into my account because I'm ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.024923</td>\n",
       "      <td>02a75041-ee65-4f45-809a-e6895777e2e8</td>\n",
       "      <td>4e4d429b-a2f1-4f30-b9e8-04e02e0f6332</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I ordered a wireless keyboard and mouse combo ...</td>\n",
       "      <td>I ordered a wireless keyboard and mouse combo ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.445748</td>\n",
       "      <td>275fb6b9-0214-4b25-a0de-ce82516f23f9</td>\n",
       "      <td>5847e5cc-d51d-4385-9924-63b3a269d9fc</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I just checked my credit card statement and I'...</td>\n",
       "      <td>I was mistakenly charged twice for a single or...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.770654</td>\n",
       "      <td>381d8ed8-be03-4936-af5d-63f2014f77f2</td>\n",
       "      <td>c5cc41d9-f8ff-4239-b3f1-085d09f72bab</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I purchased your Premium X500 laptop just 3 we...</td>\n",
       "      <td>I am experiencing constant screen flickering o...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.242457</td>\n",
       "      <td>942956f9-1fc2-4673-a1c7-d64e3c69389b</td>\n",
       "      <td>4670c6f9-e3cc-436e-be8c-95cf06b52540</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I received my order yesterday but unfortunatel...</td>\n",
       "      <td>I received the wrong item—instead of the Blue ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.282775</td>\n",
       "      <td>94a952b8-dbc7-49cc-843c-d329de765087</td>\n",
       "      <td>15cbd39b-9b19-4ec8-a13c-5bce62477d97</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How many students qualified for JEE Advanced i...</td>\n",
       "      <td>Around 2.2 lakh students qualified for JEE Adv...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.191171</td>\n",
       "      <td>d71164f3-7473-42c9-8bf3-e30413cbd6fc</td>\n",
       "      <td>9b360431-0c9b-48b4-a5df-e2140b163985</td>\n",
       "      <td>Around 2.2 lakh students qualified for JEE Adv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T18:20:20.125295Z",
     "start_time": "2025-10-04T18:20:20.120926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "JUDGE_SYSTEM_PROMPT = \"\"\"\n",
    "Please act as an impartial judge and evaluate the quality of the responses provided by two AI customer service assistants to the customer complaint below.\n",
    "Your evaluation should consider factors such as empathy, helpfulness, clarity, accuracy, tone, and how well the response addresses the customer's issue.\n",
    "Begin your evaluation by comparing the two responses and provide a short explanation.\n",
    "Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision.\n",
    "Do not favor certain assistant names.\n",
    "Be as objective as possible.\n",
    "\"\"\"\n",
    "\n",
    "JUDGE_HUMAN_PROMPT = \"\"\"\n",
    "[The Customer Complaint]\n",
    "{question}\n",
    "\n",
    "[The Start of Assistant A's Response]\n",
    "{answer_a}\n",
    "[The End of Assistant A's Response]\n",
    "\n",
    "[The Start of Assistant B's Response]\n",
    "{answer_b}\n",
    "[The End of Assistant B's Response]\n",
    "\"\"\"\n"
   ],
   "id": "4f0ebba663a936c2",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T18:20:26.542874Z",
     "start_time": "2025-10-04T18:20:26.531153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Preference(BaseModel):\n",
    "    preference: int = Field(description=\"\"\"1 if Assistant A's response is better based upon the factors above.\n",
    "2 if Assistant B's response is better based upon the factors above.\n",
    "Output 0 if it is a tie.\"\"\")\n",
    "\n",
    "def ranked_preference(inputs: dict, outputs: list[dict]) -> list:\n",
    "    completion = openai_client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": JUDGE_SYSTEM_PROMPT,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": JUDGE_HUMAN_PROMPT.format(\n",
    "                    transcript=inputs[\"transcript\"],\n",
    "                    answer_a=outputs[0].get(\"output\", \"N/A\"),\n",
    "                    answer_b=outputs[1].get(\"output\", \"N/A\")\n",
    "                )}\n",
    "        ],\n",
    "        response_format=Preference,\n",
    "    )\n",
    "\n",
    "    preference_score = completion.choices[0].message.parsed.preference\n",
    "\n",
    "    if preference_score == 1:\n",
    "        scores = [1, 0]\n",
    "    elif preference_score == 2:\n",
    "        scores = [0, 1]\n",
    "    else:\n",
    "        scores = [0, 0]\n",
    "    return scores"
   ],
   "id": "a076d725014877ec",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-04T18:20:34.556522Z",
     "start_time": "2025-10-04T18:20:27.542035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langsmith import evaluate\n",
    "\n",
    "evaluate(\n",
    "    (\"Good Summarizer-5ccd346f\", \"Bad Summarizer-cba68c37\"),  # TODO: Replace with the names/IDs of your experiments\n",
    "    evaluators=[ranked_preference]\n",
    ")"
   ],
   "id": "76d11540f0f13770",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the pairwise evaluation results at:\n",
      "https://smith.langchain.com/o/ca700a49-195a-43a4-baa7-fc421725599d/datasets/60e2c84e-8566-44ef-9612-3a494f3e8842/compare?selectedSessions=3f467d28-6282-4b9e-a8ae-8ae589a90b47%2C24ec9ca1-e777-4872-9c3d-2f37e3251085&comparativeExperiment=37c53991-fabb-463c-9bdd-2c540b2a7cd9\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'transcript'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[19]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangsmith\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m evaluate\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mGood Summarizer-5ccd346f\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mBad Summarizer-cba68c37\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# TODO: Replace with the names/IDs of your experiments\u001B[39;49;00m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mevaluators\u001B[49m\u001B[43m=\u001B[49m\u001B[43m[\u001B[49m\u001B[43mranked_preference\u001B[49m\u001B[43m]\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/_runner.py:381\u001B[39m, in \u001B[36mevaluate\u001B[39m\u001B[34m(target, data, evaluators, summary_evaluators, metadata, experiment_prefix, description, max_concurrency, num_repetitions, client, blocking, experiment, upload_results, error_handling, **kwargs)\u001B[39m\n\u001B[32m    377\u001B[39m     target_ids = [t \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(t, (\u001B[38;5;28mstr\u001B[39m, uuid.UUID)) \u001B[38;5;28;01melse\u001B[39;00m t.id \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m target]\n\u001B[32m    378\u001B[39m     logger.debug(\n\u001B[32m    379\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mRunning pairwise evaluation over existing experiments \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtarget_ids\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m...\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    380\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m381\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mevaluate_comparative\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    382\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    383\u001B[39m \u001B[43m        \u001B[49m\u001B[43mevaluators\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mSequence\u001B[49m\u001B[43m[\u001B[49m\u001B[43mCOMPARATIVE_EVALUATOR_T\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevaluators\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    384\u001B[39m \u001B[43m        \u001B[49m\u001B[43mexperiment_prefix\u001B[49m\u001B[43m=\u001B[49m\u001B[43mexperiment_prefix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    385\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdescription\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdescription\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    386\u001B[39m \u001B[43m        \u001B[49m\u001B[43mclient\u001B[49m\u001B[43m=\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    387\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    388\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    389\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    390\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m kwargs:\n\u001B[32m    391\u001B[39m     msg = (\n\u001B[32m    392\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mReceived unsupported arguments \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkwargs\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m. These arguments are not \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    393\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33msupported when creating a new experiment.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    394\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/_runner.py:975\u001B[39m, in \u001B[36mevaluate_comparative\u001B[39m\u001B[34m(experiments, evaluators, experiment_prefix, description, max_concurrency, client, metadata, load_nested, randomize_order)\u001B[39m\n\u001B[32m    973\u001B[39m         futures.append(future)\n\u001B[32m    974\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m975\u001B[39m         result = \u001B[43mevaluate_and_submit_feedback\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    976\u001B[39m \u001B[43m            \u001B[49m\u001B[43mruns_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[43mexample_id\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcomparator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexecutor\u001B[49m\n\u001B[32m    977\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    978\u001B[39m         results[example_id][\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mfeedback.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult.key\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m] = result\n\u001B[32m    979\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m futures:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/_runner.py:936\u001B[39m, in \u001B[36mevaluate_comparative.<locals>.evaluate_and_submit_feedback\u001B[39m\u001B[34m(runs_list, example, comparator, executor)\u001B[39m\n\u001B[32m    934\u001B[39m     random.shuffle(runs_list)\n\u001B[32m    935\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m rh.tracing_context(project_name=\u001B[33m\"\u001B[39m\u001B[33mevaluators\u001B[39m\u001B[33m\"\u001B[39m, client=client):\n\u001B[32m--> \u001B[39m\u001B[32m936\u001B[39m     result = \u001B[43mcomparator\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcompare_runs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mruns_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexample\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    937\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m client \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    938\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mClient is required to submit feedback.\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py:535\u001B[39m, in \u001B[36mDynamicComparisonRunEvaluator.compare_runs\u001B[39m\u001B[34m(self, runs, example)\u001B[39m\n\u001B[32m    533\u001B[39m tags = \u001B[38;5;28mself\u001B[39m._get_tags(runs)\n\u001B[32m    534\u001B[39m \u001B[38;5;66;03m# TODO: Add metadata for the \"comparison experiment\" here\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m535\u001B[39m result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    536\u001B[39m \u001B[43m    \u001B[49m\u001B[43mruns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    537\u001B[39m \u001B[43m    \u001B[49m\u001B[43mexample\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    538\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlangsmith_extra\u001B[49m\u001B[43m=\u001B[49m\u001B[43m{\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrun_id\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43msource_run_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtags\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtags\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    539\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    540\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._format_results(result, source_run_id, runs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/intro-to-langsmith/ls-academy/lib/python3.13/site-packages/langsmith/evaluation/evaluator.py:908\u001B[39m, in \u001B[36m_normalize_comparison_evaluator_func.<locals>.wrapper\u001B[39m\u001B[34m(runs, example)\u001B[39m\n\u001B[32m    904\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwrapper\u001B[39m(\n\u001B[32m    905\u001B[39m     runs: Sequence[Run], example: Optional[Example]\n\u001B[32m    906\u001B[39m ) -> _COMPARISON_OUTPUT:\n\u001B[32m    907\u001B[39m     (args, kwargs, _) = _prepare_inputs(runs, example)\n\u001B[32m--> \u001B[39m\u001B[32m908\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 19\u001B[39m, in \u001B[36mranked_preference\u001B[39m\u001B[34m(inputs, outputs)\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mranked_preference\u001B[39m(inputs: \u001B[38;5;28mdict\u001B[39m, outputs: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mdict\u001B[39m]) -> \u001B[38;5;28mlist\u001B[39m:\n\u001B[32m      9\u001B[39m     completion = openai_client.beta.chat.completions.parse(\n\u001B[32m     10\u001B[39m         model=\u001B[33m\"\u001B[39m\u001B[33mgpt-4o\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     11\u001B[39m         messages=[\n\u001B[32m     12\u001B[39m             {\n\u001B[32m     13\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mrole\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33msystem\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     14\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mcontent\u001B[39m\u001B[33m\"\u001B[39m: JUDGE_SYSTEM_PROMPT,\n\u001B[32m     15\u001B[39m             },\n\u001B[32m     16\u001B[39m             {\n\u001B[32m     17\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mrole\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33muser\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     18\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mcontent\u001B[39m\u001B[33m\"\u001B[39m: JUDGE_HUMAN_PROMPT.format(\n\u001B[32m---> \u001B[39m\u001B[32m19\u001B[39m                     transcript=\u001B[43minputs\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtranscript\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m,\n\u001B[32m     20\u001B[39m                     answer_a=outputs[\u001B[32m0\u001B[39m].get(\u001B[33m\"\u001B[39m\u001B[33moutput\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mN/A\u001B[39m\u001B[33m\"\u001B[39m),\n\u001B[32m     21\u001B[39m                     answer_b=outputs[\u001B[32m1\u001B[39m].get(\u001B[33m\"\u001B[39m\u001B[33moutput\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mN/A\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     22\u001B[39m                 )}\n\u001B[32m     23\u001B[39m         ],\n\u001B[32m     24\u001B[39m         response_format=Preference,\n\u001B[32m     25\u001B[39m     )\n\u001B[32m     27\u001B[39m     preference_score = completion.choices[\u001B[32m0\u001B[39m].message.parsed.preference\n\u001B[32m     29\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m preference_score == \u001B[32m1\u001B[39m:\n",
      "\u001B[31mKeyError\u001B[39m: 'transcript'"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "77d182ec987c8384"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ls-academy)",
   "language": "python",
   "name": "ls-academy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
